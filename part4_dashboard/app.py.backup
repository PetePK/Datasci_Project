"""
Literature Review Assistant - Streamlit Dashboard
Part 4: Interactive Web Application

Features:
- Tab 1: Search & Network Visualization
- Tab 2: Timeline Analysis
- Tab 3: Paper Details Table
- Sidebar: Controls and filters
"""

import streamlit as st
import pandas as pd
import numpy as np
import networkx as nx
from pathlib import Path
import asyncio
import time
import plotly.graph_objects as go
import plotly.express as px
from sentence_transformers import SentenceTransformer
import chromadb
import os
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import json
import hashlib
from anthropic import AsyncAnthropic

# Page config
st.set_page_config(
    page_title="Literature Review Assistant",
    page_icon="üî¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 2rem;
    }
    .stTabs [data-baseweb="tab"] {
        font-size: 1.1rem;
        font-weight: 600;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'papers' not in st.session_state:
    st.session_state.papers = None
if 'G' not in st.session_state:
    st.session_state.G = None
if 'search_performed' not in st.session_state:
    st.session_state.search_performed = False

# ============================================================================
# CACHE MANAGEMENT - SIMPLIFIED APPROACH
# ============================================================================
# Summary: Query-independent short summary ‚Üí Cache in JSON
# Suggestion: Query-specific 1-sentence highlight ‚Üí Generated fresh (no cache)

SUMMARY_CACHE_FILE = Path('../data/abstract_summaries.json')

def load_summary_cache():
    """Load abstract summary cache (query-independent)"""
    if SUMMARY_CACHE_FILE.exists():
        with open(SUMMARY_CACHE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_summary_cache(cache):
    """Save abstract summary cache to disk"""
    SUMMARY_CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(SUMMARY_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

# Suggested queries
SUGGESTED_QUERIES = [
    "machine learning for medical diagnosis",
    "climate change impacts on biodiversity",
    "renewable energy storage solutions",
    "natural language processing applications",
    "quantum computing algorithms",
    "social media impact on mental health"
]

# ============================================================================
# HELPER FUNCTIONS (from Part 3)
# ============================================================================

@st.cache_resource
def load_data():
    """Load papers, embeddings, and vector search components"""
    df = pd.read_parquet('../data/processed/papers.parquet')
    embeddings = np.load('../data/embeddings/paper_embeddings.npy')

    model = SentenceTransformer('all-MiniLM-L6-v2')
    client = chromadb.PersistentClient(path="../data/vector_db")
    collection = client.get_collection("papers")

    return df, embeddings, model, collection

def smart_search(query, model, collection, df, user_requested_count=20, min_threshold=35.0):
    """Search papers with smart threshold and fallback logic"""
    query_emb = model.encode(query)

    results = collection.query(
        query_embeddings=[query_emb.tolist()],
        n_results=min(100, len(df))
    )

    papers_data = []
    for i, (meta, distance) in enumerate(zip(results['metadatas'][0], results['distances'][0])):
        similarity = (2.0 - distance) / 2.0 * 100
        papers_data.append({
            'rank': i + 1,
            'title': meta['title'],
            'similarity': similarity,
            'distance': distance
        })

    results_df = pd.DataFrame(papers_data)
    results_df = results_df.merge(
        df[['title', 'id', 'abstract', 'year', 'citation_count', 'authors']],
        on='title',
        how='left'
    )

    title_to_idx = {title: idx for idx, title in enumerate(df['title'])}
    results_df['paper_idx'] = results_df['title'].map(title_to_idx)

    above_threshold = results_df[results_df['similarity'] >= min_threshold]

    if len(above_threshold) >= user_requested_count:
        final_papers = above_threshold.head(user_requested_count)
    else:
        final_papers = results_df.head(user_requested_count)

    return final_papers.reset_index(drop=True)

async def generate_abstract_summary(paper, client):
    """Generate query-independent short summary (cached in JSON) - ASYNC"""
    prompt = f"""Provide a brief 1-sentence summary of this research paper.

Title: {paper['title']}
Abstract: {paper['abstract']}

Write ONE concise sentence explaining what this paper is about."""

    try:
        response = await client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=100,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )
        summary = response.content[0].text.strip()
        return {'id': paper['id'], 'summary': summary}
    except Exception as e:
        print(f"Error generating summary for {paper['id']}: {e}")
        return {'id': paper['id'], 'summary': f"Summary of: {paper['title'][:100]}"}

async def analyze_all_papers(papers, api_key):
    """
    Simplified approach with Claude (ASYNC):
    - Summary: Check JSON cache ‚Üí Generate if missing ‚Üí Save to JSON (query-independent)
    """
    client = AsyncAnthropic(api_key=api_key)
    summary_cache = load_summary_cache()
    results = {}

    # Check which papers need summaries
    summary_tasks = []
    papers_need_summary = []
    for _, paper in papers.iterrows():
        paper_id = str(paper['id'])

        # Check if summary exists in cache
        if paper_id in summary_cache and not summary_cache[paper_id].startswith("Error"):
            # Use cached summary (skip if it's an error message)
            results[paper_id] = {
                'id': paper_id,
                'summary': summary_cache[paper_id]
            }
        else:
            # Need to generate summary
            papers_need_summary.append(paper)
            summary_tasks.append(generate_abstract_summary(paper, client))

    # Generate missing summaries (parallel)
    if summary_tasks:
        summary_results = await asyncio.gather(*summary_tasks)

        for paper, result in zip(papers_need_summary, summary_results):
            paper_id = str(result['id'])
            summary = result['summary']

            # Store in results
            results[paper_id] = {'id': paper_id, 'summary': summary}

            # Save to cache
            summary_cache[paper_id] = summary

        # Save summary cache
        save_summary_cache(summary_cache)

    return results

def build_similarity_network(papers_df, embeddings_full, threshold=0.60, max_edges_per_node=5):
    """Build network based on embedding similarity"""
    paper_embeddings = embeddings_full[papers_df['paper_idx'].values]
    sim_matrix = cosine_similarity(paper_embeddings)

    G = nx.Graph()

    for idx, row in papers_df.iterrows():
        G.add_node(
            row['id'],
            title=row['title'],
            abstract=row['abstract'],
            year=row['year'],
            citation_count=row['citation_count'],
            authors=row['authors'],
            relevance=row['similarity']
        )

    paper_ids = papers_df['id'].values

    for i in range(len(papers_df)):
        sims = sim_matrix[i].copy()
        sims[i] = -1
        top_k = np.argsort(sims)[::-1][:max_edges_per_node]

        for j in top_k:
            if sims[j] >= threshold:
                G.add_edge(paper_ids[i], paper_ids[j], similarity=float(sims[j] * 100))

    # Community detection
    if G.number_of_edges() > 0:
        from networkx.algorithms import community
        communities = list(community.greedy_modularity_communities(G))
        for i, comm in enumerate(communities):
            for node in comm:
                G.nodes[node]['community'] = i
    else:
        for node in G.nodes():
            G.nodes[node]['community'] = 0

    return G

# Network graph visualization removed - no longer used in dashboard

# ============================================================================
# MAIN APP
# ============================================================================

def main():
    # Show header before first search
    if not st.session_state.search_performed:
        st.markdown('<div class="main-header">üî¨ Literature Review Assistant</div>', unsafe_allow_html=True)
        st.markdown('<div class="sub-header">AI-powered research paper search with interactive visualization</div>', unsafe_allow_html=True)
        st.markdown("### üëà Use the sidebar to search research papers")

    # Sidebar
    with st.sidebar:
        st.header("üîç Search Controls")

        # Query input (use selected_query if available, otherwise use main_query or default)
        default_query = "machine learning for medical diagnosis"
        if 'selected_query' in st.session_state:
            default_query = st.session_state.selected_query
        elif not st.session_state.search_performed and 'main_search' in st.session_state:
            default_query = st.session_state.main_search

        query = st.text_input(
            "Research Question:",
            value=default_query,
            help="Enter your research question or topic"
        )

        # Number of papers slider
        num_papers = st.slider(
            "Number of Papers:",
            min_value=3,
            max_value=50,
            value=20,
            help="How many papers to display"
        )

        # Similarity threshold
        similarity_threshold = st.slider(
            "Network Similarity Threshold:",
            min_value=0.5,
            max_value=0.8,
            value=0.6,
            step=0.05,
            help="Minimum similarity to create edges (higher = fewer connections)"
        )

        # API Key input
        st.markdown("---")
        st.subheader("üîë API Configuration")
        api_key = st.text_input(
            "Anthropic API Key:",
            type="password",
            value="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",
            help="Your Claude API key for stance detection"
        )

        # Search button
        st.markdown("---")
        search_button = st.button("üöÄ Search & Analyze", type="primary", use_container_width=True)

        # Info
        st.markdown("---")
        st.info("""
        **How it works:**
        1. Enter your research question
        2. Adjust paper count and threshold
        3. Click Search & Analyze
        4. Explore results in tabs below
        """)

    # Load data
    with st.spinner("Loading data..."):
        df, embeddings_full, model, collection = load_data()

    # Search logic
    if search_button:
        if not api_key:
            st.error("‚ö†Ô∏è Please enter your Anthropic API key in the sidebar")
            return

        with st.spinner(f"Searching for '{query}'..."):
            # Step 1: Search
            progress_bar = st.progress(0)
            status_text = st.empty()

            status_text.text("üîç Searching 19K papers...")
            t1 = time.time()
            papers = smart_search(query, model, collection, df, user_requested_count=num_papers)
            search_time = time.time() - t1
            progress_bar.progress(25)

            # Step 2: Generate Summaries (Claude)
            status_text.text(f"üìù Generating summaries for {len(papers)} papers...")
            t2 = time.time()
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            analysis = loop.run_until_complete(analyze_all_papers(papers, api_key))
            loop.close()
            llm_time = time.time() - t2
            progress_bar.progress(60)

            # Add summary
            papers['summary'] = papers['id'].map(lambda x: analysis[x]['summary'])

            stance_colors = {
                'SUPPORT': '#00C853',
                'CONTRADICT': '#D32F2F',
                'NEUTRAL': '#9E9E9E'
            }
            papers['node_color'] = papers['stance'].map(stance_colors)

            # Step 3: Build network
            status_text.text("üï∏Ô∏è Building similarity network...")
            t3 = time.time()
            G = build_similarity_network(papers, embeddings_full, threshold=similarity_threshold)
            network_time = time.time() - t3
            progress_bar.progress(80)

            # Add visual attributes
            for idx, row in papers.iterrows():
                node_id = row['id']
                if node_id in G:
                    G.nodes[node_id]['color'] = row['node_color']
                    G.nodes[node_id]['size'] = max(10, row['citation_count'] * 2)
                    G.nodes[node_id]['opacity'] = row['similarity'] / 100.0
                    G.nodes[node_id]['stance'] = row['stance']
                    G.nodes[node_id]['summary'] = row['summary']

            # Complete
            progress_bar.progress(100)

            # Store in session state
            st.session_state.papers = papers
            st.session_state.G = G
            st.session_state.search_performed = True

            # Show timing breakdown
            total_time = search_time + llm_time + network_time
            status_text.text(f"‚úÖ Complete! Search: {search_time:.1f}s | LLM: {llm_time:.1f}s | Network: {network_time:.1f}s | Total: {total_time:.1f}s")
            time.sleep(2)
            progress_bar.empty()
            status_text.empty()

    # Display results
    if st.session_state.search_performed and st.session_state.papers is not None:
        papers = st.session_state.papers
        G = st.session_state.G

        # Tabs (removed Network Graph and compact search bar)
        tab1, tab2 = st.tabs(["üìà Timeline Analysis", "üìÑ Paper Details"])

        # TAB 1: Timeline Analysis (now first tab)
        with tab1:
            st.subheader("Temporal Trends Analysis")

            # Year filter
            year_range = st.slider(
                "Filter by Year Range:",
                min_value=int(papers['year'].min()),
                max_value=int(papers['year'].max()),
                value=(int(papers['year'].min()), int(papers['year'].max()))
            )

            papers_filtered = papers[(papers['year'] >= year_range[0]) & (papers['year'] <= year_range[1])]

            # Create charts in 4 columns
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                # Papers over time
                year_counts = papers_filtered['year'].value_counts().sort_index()
                fig1 = px.line(
                    x=year_counts.index,
                    y=year_counts.values,
                    markers=True,
                    title="Papers Over Time"
                )
                fig1.update_xaxes(title="Year")
                fig1.update_yaxes(title="Number of Papers")
                st.plotly_chart(fig1, use_container_width=True)

            with col2:
                # Stance distribution by year
                stance_by_year = papers_filtered.groupby(['year', 'stance']).size().unstack(fill_value=0)
                fig2 = px.bar(
                    stance_by_year,
                    title="Stance Distribution by Year",
                    color_discrete_map={'SUPPORT': '#00C853', 'NEUTRAL': '#9E9E9E', 'CONTRADICT': '#D32F2F'}
                )
                fig2.update_xaxes(title="Year")
                fig2.update_yaxes(title="Number of Papers")
                st.plotly_chart(fig2, use_container_width=True)

            with col3:
                # Average citations by year
                avg_citations = papers_filtered.groupby('year')['citation_count'].mean()
                fig3 = px.bar(
                    x=avg_citations.index,
                    y=avg_citations.values,
                    title="Avg Citations by Year",
                    color=avg_citations.values,
                    color_continuous_scale='Reds'
                )
                fig3.update_xaxes(title="Year")
                fig3.update_yaxes(title="Avg Citations")
                st.plotly_chart(fig3, use_container_width=True)

            with col4:
                # Stance distribution pie chart
                stance_counts = papers['stance'].value_counts()

                fig_pie = px.pie(
                    values=stance_counts.values,
                    names=stance_counts.index,
                    title="Overall Stance Distribution",
                    color=stance_counts.index,
                    color_discrete_map={
                        'SUPPORT': '#00C853',
                        'NEUTRAL': '#9E9E9E',
                        'CONTRADICT': '#D32F2F'
                    },
                    hole=0.4  # Donut chart style
                )

                fig_pie.update_traces(
                    textposition='inside',
                    textinfo='percent+label',
                    textfont_size=14
                )

                st.plotly_chart(fig_pie, use_container_width=True)

        # TAB 2: Paper Details (now second tab)
        with tab2:
            st.subheader("Detailed Paper Information")

            # Filters
            col1, col2 = st.columns(2)

            with col1:
                stance_filter = st.multiselect(
                    "Filter by Stance:",
                    options=['All', 'SUPPORT', 'NEUTRAL', 'CONTRADICT'],
                    default=['All']
                )

            with col2:
                sort_by = st.selectbox(
                    "Sort by:",
                    options=['Relevance', 'Citations', 'Year'],
                    index=0
                )

            # Filter papers
            if 'All' in stance_filter or len(stance_filter) == 0:
                papers_display = papers.copy()
            else:
                papers_display = papers[papers['stance'].isin(stance_filter)].copy()

            # Sort
            if sort_by == 'Relevance':
                papers_display = papers_display.sort_values('similarity', ascending=False)
            elif sort_by == 'Citations':
                papers_display = papers_display.sort_values('citation_count', ascending=False)
            elif sort_by == 'Year':
                papers_display = papers_display.sort_values('year', ascending=False)

            # Display count
            st.write(f"Showing {len(papers_display)} papers")

            # Display papers with stance colors
            for idx, row in papers_display.iterrows():
                # Color-coded stance badge
                if row['stance'] == 'SUPPORT':
                    stance_badge = "üü¢ SUPPORT"
                    badge_color = "#00C853"
                elif row['stance'] == 'CONTRADICT':
                    stance_badge = "üî¥ CONTRADICT"
                    badge_color = "#D32F2F"
                else:
                    stance_badge = "‚ö™ NEUTRAL"
                    badge_color = "#9E9E9E"

                with st.expander(f"**{stance_badge}** | {row['title']}", expanded=False):
                    # Highlighted suggestion at the top
                    st.markdown(f"<div style='background-color: {badge_color}22; padding: 10px; border-left: 4px solid {badge_color}; margin-bottom: 15px;'>"
                               f"<b>üí° {row['suggestion']}</b></div>", unsafe_allow_html=True)

                    # Summary
                    st.markdown("**üìù Summary**")
                    st.markdown(f"*{row['summary']}*")
                    st.markdown("")

                    # Metadata in compact row
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.caption(f"**Relevance:** {row['similarity']:.1f}%")
                    with col2:
                        st.caption(f"**Citations:** {row['citation_count']}")
                    with col3:
                        st.caption(f"**Year:** {row['year']}")
                    with col4:
                        # Stance badge
                        if row['stance'] == 'SUPPORT':
                            st.caption("‚úÖ **Support**")
                        elif row['stance'] == 'CONTRADICT':
                            st.caption("‚ùå **Contradict**")
                        else:
                            st.caption("‚ÑπÔ∏è **Neutral**")

                    st.markdown("---")

                    # Part 3: Full Details (toggle to view)
                    show_details = st.checkbox(f"üìÑ View Full Paper Details", key=f"details_{idx}", value=False)

                    if show_details:
                        st.markdown("**Abstract:**")
                        st.markdown(row['abstract'])
                        st.markdown("")
                        st.caption(f"**Authors:** {row['authors']}")

            # Export button
            st.markdown("---")
            csv = papers_display.to_csv(index=False)
            st.download_button(
                label="üì• Download Results as CSV",
                data=csv,
                file_name=f"papers_{query[:30].replace(' ', '_')}.csv",
                mime="text/csv",
                use_container_width=True
            )

    else:
        # Welcome screen
        st.info("""
        üëà **Get started:** Enter your research question in the sidebar and click "Search & Analyze"

        **Example queries to try:**
        - "machine learning for medical diagnosis"
        - "renewable energy storage solutions"
        - "climate change impact on biodiversity"
        - "natural language processing applications"
        """)

if __name__ == "__main__":
    main()
