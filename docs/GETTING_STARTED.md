# ğŸš€ Getting Started - Literature Review Assistant

## Quick Summary

You're building a smart literature review tool that:
1. Searches 20K papers by meaning (not keywords)
2. Shows an interactive graph
3. Uses AI to summarize papers

**Timeline**: 1 week
**Cost**: ~$2 (or $0 with free options)

---

## ğŸ“‹ What You Need

### Must Have
- Python 3.9+
- 4GB RAM
- Internet (for initial setup)

### Optional
- GPU (faster processing)
- OpenAI API key (better summaries, ~$2)

---

## âš¡ Quick Setup (15 minutes)

### Step 1: Install Python Packages

```bash
# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows

# Install everything
pip install -r requirements.txt
```

### Step 2: Download AI Models

```bash
# These run locally (free)
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
python -c "from transformers import pipeline; pipeline('text-classification', model='microsoft/deberta-v3-base-mnli')"
```

### Step 3: (Optional) Setup LLM

**Option A: Free (Ollama)**
```bash
# Download from: https://ollama.com
ollama pull llama3.1
```

**Option B: Paid ($2 total for testing)**
```bash
# Get API key from: https://platform.openai.com
echo "OPENAI_API_KEY=sk-..." > .env
```

---

## ğŸ“Š Process Your Data (1 hour)

### Parse JSON Files
```bash
python src/data/parser.py
# Output: data/processed/papers.csv
```

### Create Embeddings
```bash
python src/data/embeddings.py
# Output: data/embeddings/paper_embeddings.npy
# Takes ~5 minutes for 20K papers
```

### Load to Vector DB
```bash
python src/search/load_vector_db.py
# Output: data/vector_db/ (ChromaDB)
```

---

## ğŸ® Run the App

```bash
streamlit run dashboard/app.py
```

Open `http://localhost:8501`

---

## ğŸŒ Share with Others

### Option 1: ngrok (Quick Test)
```bash
# Install ngrok from: https://ngrok.com

# Run app
streamlit run dashboard/app.py

# In new terminal
ngrok http 8501

# Share the URL: https://abc123.ngrok-free.app
```

### Option 2: Streamlit Cloud (Permanent)
```bash
# Push to GitHub
git add .
git commit -m "Deploy app"
git push

# Go to: share.streamlit.io
# Connect repo â†’ Deploy
# Get URL: https://yourapp.streamlit.app
```

---

## ğŸ’° Cost Breakdown

### Free Option ($0)
```
âœ“ sentence-transformers (embeddings)
âœ“ ChromaDB (vector search)
âœ“ DeBERTa NLI (stance detection)
âœ“ Ollama + Llama 3.1 (summaries)
âœ“ Streamlit Cloud (hosting)

Total: $0
Trade-off: Slower summaries (5-10 sec each)
```

### Recommended ($2)
```
âœ“ Everything above, BUT:
âœ“ OpenAI GPT-4o-mini (summaries)

Cost: $0.15 per 1M tokens
Usage: 5 users Ã— 100 searches Ã— 10 summaries = 5000 summaries
Cost: 5000 Ã— $0.001 = $5 (max)
Realistic: ~$1-2

Trade-off: Fast (2 sec), better quality
```

---

## ğŸ“ File Structure (After Setup)

```
Datasci_Project/
â”œâ”€â”€ raw_data/                    # Your 20K JSON files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ papers.csv           # âœ“ After step 1
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ paper_embeddings.npy # âœ“ After step 2
â”‚   â””â”€â”€ vector_db/               # âœ“ After step 3
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ parser.py            # Run this first
â”‚   â”‚   â””â”€â”€ embeddings.py        # Run this second
â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”œâ”€â”€ load_vector_db.py    # Run this third
â”‚   â”‚   â”œâ”€â”€ vector_search.py     # Used by dashboard
â”‚   â”‚   â””â”€â”€ stance_detection.py  # Used by dashboard
â”‚   â””â”€â”€ llm/
â”‚       â””â”€â”€ summarizer.py        # Used by dashboard
â””â”€â”€ dashboard/
    â””â”€â”€ app.py                   # Run this to start
```

---

## ğŸ¯ 7-Day Plan

### Day 1 (4 hours)
- âœ“ Setup environment
- âœ“ Parse JSON â†’ CSV
- âœ“ Data exploration

### Day 2 (5 hours)
- âœ“ Create embeddings
- âœ“ Load to ChromaDB
- âœ“ Test search

### Day 3 (5 hours)
- âœ“ Add relevance scoring
- âœ“ Add stance detection (NLI)
- âœ“ Test both

### Day 4 (5 hours)
- âœ“ Build citation network
- âœ“ Setup LLM summarization
- âœ“ Test graph

### Day 5 (5 hours)
- âœ“ Build Streamlit dashboard
- âœ“ Search interface
- âœ“ Graph visualization

### Day 6 (5 hours)
- âœ“ Add summary panel
- âœ“ Add report generator
- âœ“ Polish UI

### Day 7 (4 hours)
- âœ“ Deploy (ngrok or Streamlit Cloud)
- âœ“ Test with 5 users
- âœ“ Record demo video

---

## ğŸ› Troubleshooting

### "Out of memory" error
```bash
# Process in batches
# Edit src/data/embeddings.py
# Change batch_size from 32 to 16 or 8
```

### NLI model is slow
```bash
# Use GPU if available
# Or reduce number of papers analyzed
# Only analyze top 20 instead of 50
```

### Streamlit Cloud deployment fails
```bash
# Check requirements.txt has all dependencies
# Make sure data/vector_db/ is included
# Or regenerate it on first run
```

### OpenAI API costs too much
```bash
# Switch to Ollama (free)
# Edit src/llm/summarizer.py
# Use Ollama instead of OpenAI
```

---

## ğŸ“š Key Files to Read

1. **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)** - Complete guide with all options
2. **[CONCEPT_SUMMARY.md](CONCEPT_SUMMARY.md)** - Quick concept overview
3. **[requirements.txt](requirements.txt)** - All dependencies
4. **[README.md](README.md)** - Project overview

---

## âœ… Checklist

Before you start:
- [ ] Python 3.9+ installed
- [ ] Git installed
- [ ] 5GB free disk space
- [ ] Internet connection

After setup:
- [ ] Virtual environment activated
- [ ] All packages installed
- [ ] Models downloaded
- [ ] papers.csv exists
- [ ] Embeddings created
- [ ] Vector DB loaded
- [ ] Dashboard runs locally

Ready to share:
- [ ] Dashboard works on localhost
- [ ] Can search and get results
- [ ] Graph displays correctly
- [ ] Summaries work
- [ ] Deployed (ngrok or Streamlit Cloud)
- [ ] 5 users can access

---

## ğŸ†˜ Need Help?

### Common Questions

**Q: Do I need GPU?**
A: No, but it's faster. CPU works fine (just slower).

**Q: Can I use free options only?**
A: Yes! Use Ollama instead of OpenAI.

**Q: How long does setup take?**
A: 15 min install + 1 hour data processing = ~1.5 hours

**Q: Will it work with more/fewer papers?**
A: Yes! Works with 1K to 100K papers.

**Q: Can I customize the graph colors?**
A: Yes! Edit `dashboard/app.py`

---

## ğŸ“ Learning Resources

- **Vector Search**: Read [docs/RAG_EXPLAINED.md](docs/RAG_EXPLAINED.md)
- **ChromaDB**: https://docs.trychroma.com/
- **Streamlit**: https://docs.streamlit.io/
- **sentence-transformers**: https://www.sbert.net/

---

## ğŸš€ Ready to Start?

```bash
# 1. Clone repo (if not done)
git clone https://github.com/PetePK/Datasci_Project.git
cd Datasci_Project

# 2. Setup
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt

# 3. Process data
python src/data/parser.py
python src/data/embeddings.py
python src/search/load_vector_db.py

# 4. Run
streamlit run dashboard/app.py

# 5. Share
ngrok http 8501
```

**That's it! You're ready to go!** ğŸ‰

---

**Questions? Check [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md) for detailed explanations of every step.**
